{"id": "445a6bb9-8c47-36ad-85ea-23af25fd640f", "title": "The Fight for the Future of YouTube", "url": "https://www.newyorker.com/tech/annals-of-technology/the-fight-for-the-future-of-youtube", "summary": "Schaffer told me that hate speech had been a problem on YouTube since its earliest days.\nIn theory, a platform might fine-tune its algorithms to de\u00ebmphasize hate speech and conspiracy theories.\nMachine-learning systems struggle to tell the difference between actual hate speech and content that describes or contests it.\nMany outside researchers argue that this system, which helped drive YouTube\u2019s engagement growth, also amplified hate speech and conspiracy theories on the platform.\nThe virality of YouTube videos has long been driven by imitation: in the site\u2019s early days, clips such as \u201cCrazy frog brothers\u201d and \u201cDavid After Dentist\u201d led fans and parodists to re\u00ebnact their every move.", "paragraphs": ["Earlier this year, executives at YouTube began mulling, once again, the problem of online speech. On grounds of freedom of expression and ideological neutrality, the platform has long allowed users to upload videos endorsing noxious ideas, from conspiracy theories to neo-Nazism. Now it wanted to reverse course. \u201cThere are no sacred cows,\u201d Susan Wojcicki, the C.E.O. of YouTube, reportedly told her team. Wojcicki had two competing goals: she wanted to avoid accusations of ideological bias while also affirming her company\u2019s values. In the course of the spring, YouTube drafted a new policy that would ban videos trafficking in historical \u201cdenialism\u201d (of the Holocaust, 9/11, Sandy Hook) and \u201csupremacist\u201d views (lauding the \u201cwhite race,\u201d arguing that men were intellectually superior to women). YouTube planned to roll out its new policy as early as June. In May, meanwhile, it started preparing for Pride Month, turning its red logo rainbow-colored and promoting popular L.G.B.T.Q. video producers on Instagram.", "Susan Wojcicki, the C.E.O. of YouTube. The company is facing profound technical, social, and political challenges in moderating its own system. Photograph by Peter Prato / NYT / Redux", "On May 30th, Carlos Maza, a media critic at Vox, upended these efforts. In a Twitter thread that quickly went viral, Maza argued that the company\u2019s publicity campaign belied its lax enforcement of the content and harassment policies it had already put in place. Maza posted a video supercut of bigoted insults that he\u2019d received from Steven Crowder, a conservative comedian with nearly four million YouTube followers; the insults focussed on Maza\u2019s ethnicity and sexual orientation. When Crowder mentioned Maza in a video, his fans piled on; last year, Maza\u2019s cell phone was bombarded with hundreds of texts from different numbers which read \u201cdebate steven crowder.\u201d Maza said that he\u2019d reported the behavior to YouTube\u2019s content moderators numerous times, and that they had done nothing.", "On Twitter and his YouTube channel, Crowder insisted that, in labelling Maza a \u201clispy queer\u201d and a \u201ctoken Vox gay-athiest sprite,\u201d he had been trying to be funny. Maza\u2019s supporters, meanwhile, shared screenshots of ads that had run before Crowder\u2019s videos, suggesting that, because YouTube offers popular video producers a cut of ad revenue, the company had implicitly condoned Crowder\u2019s messages. YouTube said it would investigate. A week later, it tweeted that Crowder hadn\u2019t violated its community guidelines in any of the videos that Maza highlighted. The next day, it announced its new policy, which included a warning that the company would no longer share ad revenue with YouTubers who repeatedly brushed up against its rules. Then it announced that Crowder would be cut off from the platform\u2019s ad dollars.", "The news made no one happy. Maza said that he wanted Crowder\u2019s channel removed completely; conservatives, including the Republican senator Ted Cruz, complained about censorship. YouTube employees, siding with Maza, began denouncing their bosses on Twitter and in the press. \u201cIt\u2019s a classic move from a comms playbook,\u201d Micah Schaffer, a technology adviser who wrote YouTube\u2019s first community guidelines, told me. \u201cLike, \u2018Hey, can we move up that launch to change the news cycle?\u2019 Instead, it made it worse. It combined into a Voltron of bad news.\u201d (A YouTube spokesperson said that the launch date was not in response to any individual event.) Former colleagues deluged Schaffer, who had left the company in 2009, with bewildered e-mails and texts. (A typical subject line: \u201cWTF is Going on at YouTube?\u201d) Sitting in a dentist\u2019s office, he started typing a response on his phone, trying to lay out what he thought had gone wrong at the company.", "Schaffer told me that hate speech had been a problem on YouTube since its earliest days. Dealing with it used to be fairly straightforward. YouTube was founded, in 2005, by Chad Hurley, Steve Chen, and Jawed Karim, who met while working at PayPal. At first, the site was moderated largely by its co-founders; in 2006, they hired a single, part-time moderator. The company removed videos often, rarely encountering pushback. In the intervening thirteen years, a lot has changed. \u201cYouTube has the scale of the entire Internet,\u201d Sundar Pichai, the C.E.O. of Google, which owns YouTube, told Axios last month. The site now attracts a monthly audience of two billion people and employs thousands of moderators. Every minute, its users upload five hundred hours of new video. The technical, social, and political challenges of moderating such a system are profound. They raise fundamental questions not just about YouTube\u2019s business but about what social-media platforms have become and what they should be.", "Perhaps because of the vast scale at which most social platforms operate, proposed solutions to the problem of online hate speech tend to be technical in nature. In theory, a platform might fine-tune its algorithms to de\u00ebmphasize hate speech and conspiracy theories. But, in practice, this is harder than it sounds. Some overtly hateful users may employ language and symbols that clearly violate a site\u2019s community guidelines\u2014but so called borderline content, which dances at the edge of provocation, is harder to detect and draws a broad audience. Machine-learning systems struggle to tell the difference between actual hate speech and content that describes or contests it. (After YouTube announced its new policies, the Southern Poverty Law Center complained that one of its videos, which was meant to document hate speech, had been taken down.) Some automated systems use metadata\u2014information about how often a user posts, or about the number of comments that a post gets in a short period of time\u2014to flag toxic content without trying to interpret it. But this sort of analysis is limited by the way that content bounces between platforms, obscuring the full range of interactions it has provoked.", "Tech companies have hired thousands of human moderators to make nuanced decisions about speech. YouTube also relies on anonymous outside \u201craters\u201d to evaluate videos and help train its recommendations systems. But the flood of questionable posts is overwhelming, and sifting through it can take a psychological toll. Earlier this year, YouTube described its efforts to draw more heavily on user feedback\u2014survey responses, likes and dislikes\u2014to help identify \u201cquality\u201d videos. And yet, in a 2016 white paper, the company\u2019s own engineers wrote that such metrics aren\u2019t very useful; the problem is that, for many videos, \u201cexplicit feedback is extremely sparse\u201d compared to \u201cimplicit\u201d signals, such as what users click on or how long they watch a video. Teen-agers, in particular, who use YouTube more than any other kind of social media, often respond to surveys in mischievous ways.", "Business challenges compound the technical ones. In a broad sense, any algorithmic change that dampens user engagement could work against YouTube\u2019s business model. Netflix, which is YouTube\u2019s chief rival in online video, can keep subscribers streaming by licensing or crafting addictive content; YouTube, by contrast, relies on user-generated clips, strung together by an automated recommendation engine. Programmers are always tweaking the system and the company is reluctant to disclose details. Still, a 2018 white paper outlined the general principle at that time: once someone starts watching a video, the engine is designed to \u201cdig into a topic more deeply,\u201d luring the viewer down the proverbial rabbit hole. Many outside researchers argue that this system, which helped drive YouTube\u2019s engagement growth, also amplified hate speech and conspiracy theories on the platform. As the engine dug deeper, it risked making unsavory suggestions: unearth enough videos about the moon landing and some of them may argue that it was faked.", "Francesca Tripodi, a media scholar at James Madison University, has studied how right-wing conspiracy theorists perpetuate false ideas online. Essentially, they find unfilled rabbit holes and then create content to fill them. \u201cWhen there is limited or no metadata matching a particular topic,\u201d she told a Senate committee in April, \u201cit is easy to co\u00f6rdinate around keywords to guarantee the kind of information Google will return.\u201d Political provocateurs can take advantage of data vacuums to increase the likelihood that legitimate news clips will be followed by their videos. And, because controversial or outlandish videos tend to be riveting, even for those who dislike them, they can register as \u201cengaging\u201d to a recommendation system, which would surface them more often. The many automated systems within a social platform can be co-opted and made to work at cross purposes.", "Technological solutions are appealing, in part, because they are relatively unobtrusive. Programmers like the idea of solving thorny problems elegantly, behind the scenes. For users, meanwhile, the value of social-media platforms lies partly in their appearance of democratic openness. It\u2019s nice to imagine that the content is made by the people, for the people, and that popularity flows from the grass roots.", "In fact, the apparent democratic neutrality of social-media platforms has always been shaped by algorithms and managers. In its early days, YouTube staffers often cultivated popularity by hand, choosing trending videos to highlight on its home page; if the site gave a leg up to a promising YouTuber, that YouTuber\u2019s audience grew. By spotlighting its most appealing users, the platform attracted new ones. It also shaped its identity: by featuring some kinds of content more than others, the company showed YouTubers what kind of videos it was willing to boost. \u201cThey had to be super family friendly, not copyright-infringing, and, at the same time, compelling,\u201d Schaffer recalled, of the highlighted videos.", "Today, YouTube employs scores of \u201cpartner managers,\u201d who actively court and promote celebrities, musicians, and gamers\u2014meeting with individual video producers to answer questions about how they can reach bigger audiences, giving them early access to new platform features, and inviting them to workshops where they can network with other successful YouTubers. Since 2016, meanwhile, it has begun paying socially conscious YouTubers to create videos about politically charged subjects, through a program called Creators for Change. \u201cIn this instance, it\u2019s a social-impact group,\u201d Paul Marvucic, a YouTube marketing manager, explained. \u201cWe\u2019re saying, \u2018We really believe in what you guys are saying, and it\u2019s very core to our values.\u2019 \u201d", "The question of YouTube\u2019s values\u2014what they are, whether it should have them, how it should uphold them\u2014is fraught. In December of last year, Sundar Pichai, the C.E.O. of Google, went before Congress and faced questions about social media\u2019s influence on politics. Democrats complained that YouTube videos promoted white supremacy and right-wing extremism; Republicans, in turn, worried that the site might be \u201cbiased\u201d against them, and that innocent videos might be labelled as hate speech merely for containing conservative views. \u201cIt\u2019s really important to me that we approach our work in an unbiased way,\u201d Pichai said.", "And yet the Creators for Change program requires YouTube to embrace certain kinds of ideological commitments. This past fall, for an audience of high-school and college students, YouTube staged a Creators for Change event in the Economic and Social Council chamber at the United Nations. The occasion marked the seventieth anniversary of the Universal Declaration of Human Rights, and five \u201cambassadors\u201d from the program joined Craig Mokhiber, the director of the New York office of the U.N. High Commissioner for Human Rights, onstage. \u201cThe U.N. is not just a conference center that convenes to hear any perspective offered by any person on any issue,\u201d Mokhiber said. Instead, he argued, it represents one side in a conflict of ideas. In one corner are universal rights to housing, health care, education, food, and safety; in the other are the ideologies espoused by Islamophobes, homophobes, anti-Semites, sexists, ethno-nationalists, white supremacists, and neo-Nazis. In his view, YouTube needed to pick a side. He urged the YouTubers onstage to take the ideals represented by the U.N. and \u201camplify\u201d them in their videos. \u201cWe\u2019re in the middle of a struggle that will determine, in our lifetime, whether human dignity will be advanced or crushed, for us and for future generations,\u201d he said.", "Last year, YouTube paid forty-seven ambassadors to produce socially conscious videos and attend workshops. The program\u2019s budget, of around five million dollars\u2014it also helps fund school programs designed to improve students\u2019 critical-thinking skills when they are confronted with emotionally charged videos\u2014is a tiny sum compared to the hundreds of millions that the company reportedly spends on YouTube Originals, its entertainment-production arm. Still, one YouTube representative told me, \u201cWe saw hundreds of millions of views on ambassadors\u2019 videos last year\u2014hundreds of thousands of hours of watch time.\u201d Most people encountered the Creators for Change clips as automated advertisements before other videos.", "The Mumbai-based comedian Prajakta Koli, known on YouTube as MostlySane, sat beside Mokhiber in the U.N. chamber. Around four million people follow her channel. Her videos usually riff on the irritating people whom she encounters in her college cafeteria or on the pitfalls of dating foreigners. \u201cNo Offence,\u201d a music video that she screened at the Creators for Change event, is different. As it begins, Koli slouches in her pajamas on the couch, watching a homophobe, a misogynist, and an Internet troll\u2014all played by her\u2014rant on fictional news shows. A minute later, she dons boxing gloves and takes on each of them in a rap battle. After the screening, Koli said that she had already begun taking on weighty subjects, such as divorce and body shaming, on her own. But it helped that YouTube had footed the production and marketing costs for \u201cNo Offence,\u201d which were substantial. The video is now her most watched, with twelve million views.", "On a channel called AsapScience, Gregory Brown, a former high-school teacher, and his boyfriend, Mitchell Moffit, make animated clips about science that affects their viewers\u2019 everyday lives; their most successful videos address topics such as the science of coffee or masturbation. They used their Creators for Change dollars to produce a video about the scientifically measurable effects of racism, featuring the Black Lives Matter activist DeRay Mckesson. While the average AsapScience video takes a week to make, the video about racism had taken seven or eight months: the level of bad faith and misinformation surrounding the topic, Brown said, demanded extra precision. \u201cYou need to explain the study, explain the parameters, and explain the result so that people can\u2019t argue against it,\u201d he said. \u201cAnd that doesn\u2019t make the video as interesting, and that\u2019s a challenge.\u201d (Toxic content proliferates, in part, because it is comparatively easy and cheap to make; it can shirk the burden of being true.)", "YouTube hopes that Creators for Change will have a role-model effect. The virality of YouTube videos has long been driven by imitation: in the site\u2019s early days, clips such as \u201cCrazy frog brothers\u201d and \u201cDavid After Dentist\u201d led fans and parodists to re\u00ebnact their every move. When it comes to political videos, imitation has cut both ways. The perceived popularity of conspiracy videos may have led some YouTubers to make similar clips; conversely, many Creators for Change ambassadors cite other progressive YouTubers as inspirations. (Prajakta Koli based her sketches on those of Lilly Singh, a sketch-comedy YouTuber who has also spoken at the United Nations.) In theory, even just broadcasting the idea that YouTube will reward social-justice content with production dollars and free marketing might encourage a proliferation of videos that denounce hate speech."], "authors": ["Neima Jahromi", "Neima Jahrom", "Noam Cohe", "Sue Halper"], "keywords": ["youtube", "videos", "fight", "speech", "maza", "hate", "future", "video", "creators", "change", "youtubers", "content"], "pubDate": null, "publicationId": "7e9b9ffe-e645-302d-9d94-517670623b35"}